{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to provide an overview of the Transformer architecture, focusing on its core components: encoders, decoders, and the combined encoder-decoder structure. The video aims to explain these concepts in simple, high-level terms and does not require prior knowledge of neural networks, although a basic understanding of vectors and tensors may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder is responsible for converting textual inputs into numerical representations, often referred to as embeddings or features. It mainly employs the self-attention mechanism and has bi-directional properties. \n",
    "\n",
    "![Encoders](./img/encoder.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such specialized task is causal language modeling, which involves predicting the next word in a sequence based solely on its preceding and current words. This is called \"causal\" because the prediction is contingent on past and present inputs, without any knowledge of future context. This task exemplifies how a model's generalized capabilities can be fine-tuned to perform more specialized operations.\n",
    "\n",
    "![Causal Language Modelling](./img/causal_language.jpg)\n",
    "\n",
    "Another example is masked language modelling, whick predicts a masked word in the sentence. \n",
    "\n",
    "![Masked Language Modelling](./img/mask.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decoder\n",
    "The decoder can also accept textual inputs and uses a mechanism similar to the encoder, known as masked self-attention. However, it differs from the encoder in being uni-directional and is traditionally employed in an auto-regressive manner.\n",
    "\n",
    "![Decoders](./img/decoder.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encoder-Decoder\n",
    "When combined, the encoder and decoder form a sequence-to-sequence transformer. The encoder processes inputs into a high-level representation, which the decoder then uses, along with other inputs, to generate predictions in an auto-regressive fashion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
