{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The Vision Transformer (ViT) pioneered the elimination of convolutions in computer vision tasks, opting for a standard Transformer encoder instead. ViT processes an image by dissecting it into uniform patches and generating embeddings, much like how textual sentences are tokenized. This methodology allowed ViT to compete effectively with existing CNNs while being more resource-efficient. Models that followed ViT expanded its capabilities to address dense vision tasks, such as segmentation and detection.\n",
    "\n",
    "Swin Transformer takes it a step further by constructing hierarchical features from small patches and subsequently merging them with adjacent patches in deeper layers. This model restricts attention to a localized window, which then shifts across layers to establish beneficial learning connections. Given its proficiency in generating hierarchical features, Swin Transformer is particularly useful for dense prediction challenges like segmentation and detection. On the other hand, SegFormer also employs a Transformer encoder to build hierarchical features, but augments it with a multilayer perceptron (MLP) decoder for final prediction tasks.\n",
    "\n",
    "Influenced by BERT's pretraining objectives, other models like BeIT and ViTMAE have emerged. BeIT is trained through masked image modeling (MIM), predicting visual tokens for randomly obscured image patches. ViTMAE shares a similar goal but focuses on predicting the actual pixels of the masked areas. Remarkably, 75% of its image patches are masked during training. After the pretraining phase, the decoder is discarded, and the encoder becomes ready for application in downstream tasks.\n",
    "\n",
    "### Decoder\n",
    "Vision models that exclusively rely on decoders are relatively rare, mainly because the task of image representation is often left to encoders. However, in specialized tasks like image generation, decoders find their niche. ImageGPT, for instance, adapts the GPT-2 architecture to predict the subsequent pixel in an image series instead of the next text token.\n",
    "\n",
    "### Encoder-Decoder\n",
    "Typically, vision models utilize an encoder, often referred to as a backbone, for the extraction of pivotal image features. These features are then fed into a Transformer decoder for further processing. DETR stands out in this category with its complete Transformer encoder-decoder architecture tailored for object detection. The encoder extracts image features and blends them with object queries in the decoder. Each object query is a specialized embedding that targets a specific region or object within the image. DETR then forecasts the bounding box parameters and class labels corresponding to each object query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
