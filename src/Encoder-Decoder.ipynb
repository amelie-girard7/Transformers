{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to provide an overview of the Transformer architecture, focusing on its core components: encoders, decoders, and the combined encoder-decoder structure. The video aims to explain these concepts in simple, high-level terms and does not require prior knowledge of neural networks, although a basic understanding of vectors and tensors may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder is responsible for converting textual inputs into numerical representations, often referred to as embeddings or features. It mainly employs the self-attention mechanism and has bi-directional properties. \n",
    "\n",
    "![Encoders](./img/encoder.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decoder\n",
    "The decoder can also accept textual inputs and uses a mechanism similar to the encoder, known as masked self-attention. However, it differs from the encoder in being uni-directional and is traditionally employed in an auto-regressive manner.\n",
    "\n",
    "![Decoders](./img/decoder.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encoder-Decoder\n",
    "When combined, the encoder and decoder form a sequence-to-sequence transformer. The encoder processes inputs into a high-level representation, which the decoder then uses, along with other inputs, to generate predictions in an auto-regressive fashion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
