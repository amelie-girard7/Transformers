{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Hands-On Example : Text Classification for customer feedback to evaluate the level of their satisfaction.\n"," This is hands-on example for text classification using DistilBERT, a distilled version of BERT. The example demonstrates how to use the Hugging Face Transformers library for this task."]},{"cell_type":"markdown","metadata":{},"source":["##### Import Required Libraries"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["- DistilBertTokenizer: This is used for converting text into tokens that the DistilBERT model can understand.\n","- DistilBertForSequenceClassification: This is the actual DistilBERT model designed for text classification.\n","- torch: PyTorch library, required for various tensor operations."]},{"cell_type":"markdown","metadata":{},"source":["#### intialise the tokenizer and model "]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["- from_pretrained('distilbert-base-uncased'): This loads the pre-trained DistilBERT model and tokenizer. The 'uncased' version means that the text will be converted to lowercase."]},{"cell_type":"markdown","metadata":{},"source":["### Tokenize the Text"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["text = \"Iam not statisfied with the product. It is not working as expected.\"\n","inputs = tokenizer(text, return_tensors=\"pt\")"]},{"cell_type":"markdown","metadata":{},"source":["- text: The sample text you want to classify.\n","- tokenizer(text, return_tensors=\"pt\"): This converts the sample text into tokens. \n","\n","The return_tensors=\"pt\" argument indicates that the output should be PyTorch tensors."]},{"cell_type":"markdown","metadata":{},"source":["### Run the Model"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[],"source":["outputs = model(**inputs)\n","logits = outputs.logits"]},{"cell_type":"markdown","metadata":{},"source":["- model(**inputs): This runs the tokenized text through the DistilBERT model.\n","\n","- outputs.logits: The model returns logits, which are raw, unnormalized scores for each class in the classification task."]},{"cell_type":"markdown","metadata":{},"source":["### Make Predictions"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","probs = F.softmax(logits, dim=1)\n","prediction = torch.argmax(probs, dim=1)"]},{"cell_type":"markdown","metadata":{},"source":["- F.softmax(logits, dim=1): This applies the Softmax function to convert the logits into probabilities.\n","\n","- torch.argmax(probs, dim=1): This finds the index of the maximum value in the probabilities tensor, effectively giving you the predicted class."]},{"cell_type":"markdown","metadata":{},"source":["### Print the Prediction"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted Label Description: The review indicates customer dissatisfaction and negative feedback.\n"]}],"source":["# Sample label mapping with descriptive sentences\n","label_mapping = {\n","    0: \"The review indicates customer satisfaction and positive feedback.\",\n","    1: \"The review indicates customer dissatisfaction and negative feedback.\",\n","    2: \"The review is neutral and does not indicate either satisfaction or dissatisfaction.\"\n","}\n","\n","# Prediction tensor to integer\n","predicted_label_index = prediction.item()\n","\n","# Map the prediction to the label\n","predicted_label = label_mapping[predicted_label_index]\n","\n","print(f\"Predicted Label Description: {predicted_label}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["This prints the prediction, which will be the index of the class with the highest probability.\n","\n","That's the entire workflow for text classification using DistilBERT. This example is basic and serves educational purposes. In a real-world application, you'd also have additional steps like data preprocessing, model training, and evaluation."]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
